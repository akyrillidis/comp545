<h2 align="center"><b> Advanced topics in optimization: From simple to complex ML systems</b> </h2>

<br>
<br>

<table style="width:100%">  
  <tr>
    <td>Email (instructor): anastasios@rice.edu</td>
    <td align="right">Web: https://akyrillidis.github.io/comp545/</td> 
  </tr>
  <tr>
    <td> </td>
    <td align="right">Email (course): RiceCOMP545@gmail.com</td> 
  </tr>
  <tr>
    <td>Office hours: By appointment </td>
    <td align="right">Class hours: T\TH 14:30 - 15:45</td> 
  </tr>
  <tr>
    <td>Office: DH 3119</td>
    <td align="right">Classroom: AEL B209 </td> 
  </tr>
</table>

<table style="width:100%">  
  <tr> 
    <td align="center"><a href="comp545/Syllabus.pdf">Course Syllabus</a></td>
    <td align="center"><a href="https://drive.google.com/open?id=1pkU1l22Exqj-0AxCvvq5xSvAMpxiVAVi">LaTEX template for scribing</a></td>
  </tr>
</table>

<table style="width:100%">  
  <tr> 
    <td align="left"><a href="http://akyrillidis.github.io/comp545/">Course description</a></td>
    <td align="left"><a href="http://akyrillidis.github.io/comp545/schedule.html">Schedule</a></td> 
    <td align="left"><a href="http://akyrillidis.github.io/comp545/grading.html">Grading policy</a></td> 
    <td align="left"><a href="http://akyrillidis.github.io/comp545/literature.html">Literature</a></td> 
  </tr>
</table>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 1.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><b>- Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares</b>, Stephen Boyd, Lieven Vandenberghe</td>
  </tr>
  <br>
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="https://stanford.edu/~jduchi/projects/DuchiShSiCh08.pdf">- Efficient Projections onto the \ell_1-Ball for Learning in High Dimensions</a></b></td>    
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="http://proceedings.mlr.press/v37/asteris15.pdf">- Stay on path: PCA along graph paths</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.pnas.org/content/106/3/697.full.pdf">- CUR matrix decompositions for improved data analysis</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1206.0594.pdf">- Simple and Deterministic Matrix Sketching</a></b></td> 
  </tr> 
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 2.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.855&rep=rep1&type=pdf"><b>- Introductory lectures on convex optimization (Sections 1.2, 1.3, 2.1)</b></a>, Yurii Nesterov</td>
  </tr>
  
  <tr>
    <td></td>
    <td align="left"><b><a href="http://sbubeck.com/Bubeck15.pdf">- Convex Optimization: Algorithms and Complexity (Sections 3.1, 3.2, 3.4, 3.5)</a></b>, Sebastien Bubeck</td>
  </tr>
  
  <br>
  
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="https://arxiv.org/abs/1608.04636">- Linear convergence of gradient and proximal-gradient methods under the Polyak-≈Åojasiewicz condition</a></b></td>    
  </tr>  
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 3.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.855&rep=rep1&type=pdf"><b>- Introductory lectures on convex optimization (Sections 1.2, 2.2)</b></a>, Yurii Nesterov</td>
  </tr>
  
  <tr>
    <td></td>
    <td align="left"><b>- Numerical optimization (Sections 8.1, 8.2) </b>, Jorge Nocedal and Stephen Wright</td>
  </tr>
  
  <tr>
    <td></td>
  <td align="left"><b><a href="https://leon.bottou.org/publications/pdf/tr-optml-2016.pdf">- Optimization Methods for Large-Scale Machine Learning</a></b>, Leon Bottou, Frank Curtis, and Jorge Nocedal</td>
  </tr>
  
  <br>
  
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="http://www.ams.org/journals/mcom/1970-24-109/S0025-5718-1970-0258249-6/S0025-5718-1970-0258249-6.pdf">- A Family of Variable-Metric Methods Derived by Variational Means</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="ftp://ftp.numerical.rl.ac.uk/pub/nimg/pubs_old/ConnGoulToin91_mp.pdf">- Convergence of quasi-Newton matrices generated by the symmetric rank one update</a></b></td>   
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://pdfs.semanticscholar.org/8427/2faaaf0074b461570e5bb48514ac2c94aa72.pdf">- Random Gradient-Free Minimization of Convex Functions</a></b></td>  
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1802.05666.pdf">- Adversarial Risk and the Dangers of Evaluating Against Weak Attacks</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://www.sciencedirect.com/science/article/abs/pii/0041555364901375">- Some methods of speeding up the convergence of iteration methods</a></b></td>   
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="http://mpawankumar.info/teaching/cdt-big-data/nesterov83.pdf">- A method for solving the convex programming problem with convergence rate O(1/k^2)</a></b></td>     
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://statweb.stanford.edu/~candes/papers/adap_restart_paper.pdf">- Adaptive Restart for Accelerated Gradient Schemes</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://distill.pub/2017/momentum/">- Why Momentum Really Works</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="http://tongzhang-ml.org/papers/icml04-stograd.pdf">- Solving large-scale linear prediction problems using SGD</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://www2.isye.gatech.edu/~nemirovs/SIOPT_RSA_2009.pdf">- Robust stochastic approximation approach to stochastic programming</a></b></td>    
  </tr>   
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1310.5715.pdf">- Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.optimization-online.org/DB_FILE/2014/12/4679.pdf">- Coordinate descent algorithms</a></b></td>    
  </tr> 
  
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 4.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.855&rep=rep1&type=pdf"><b>- Introductory lectures on convex optimization (Sections 1.2, 2.1, 2.2)</b></a>, Yurii Nesterov</td>
  </tr>
    
  <br>
  
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf">- Stochastic Gradient Descent Tricks</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf">- Early stopping - but when?</a></b></td>  
  </tr>  
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Presentations #1</b></td>
    <td align="left"><b><a href="http://m8j.net/math/revisited-FW.pdf">- Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf">- Deep learning via Hessian-free optimization</a></b></td>   
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf">- On the importance of initialization and momentum in deep learning</a></b></td>  
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1609.04836.pdf">- On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a></b></td>    
  </tr>  
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 5.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><a href="https://arxiv.org/abs/1712.07897f"><b>- Non-convex optimization for machine learning (Section 7)</b></a>, Prateek Jain, Purushottam Kar</td>
  </tr>
  <tr>
    <td></td>
    <td align="left"><b>- A mathematical introduction to compressive sensing</b>, Simon Foucart, Holger Rauhut</td>
  </tr>
    
  <br>
  
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="http://www.math.yorku.ca/~hkj/Teaching/6621Winter2013/Coverage/lasso.pdf">- Regression Shrinkage and Selection via the Lasso</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://web.stanford.edu/group/SOL/papers/BasisPursuit-SIGEST.pdf">- Atomic Decomposition by Basis Pursuit</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/0805.0510.pdf">- Iterative hard thresholding for compressed sensing</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/0909.0777.pdf">- Optimally Tuned Iterative Reconstruction
Algorithms for Compressed Sensing</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://statweb.stanford.edu/~candes/papers/RIP.pdf">- The Restricted Isometry Property
and Its Implications for Compressed Sensing</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="http://imi.cas.sc.edu/MURIwebsite/publications/publications-pertaining-to-this-muri/JLCSfinalrevision1.pdf">- A Simple Proof of the Restricted Isometry Property for Random Matrices</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://icml.cc/Conferences/2009/papers/115.pdf">- Gradient Descent with Sparsification: An iterative algorithm for sparse recovery with restricted isometry property</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://infoscience.epfl.ch/record/183059/files/CAMSAP_Recipes_for_HTM.pdf">- Recipes on hard thresholding methods</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1104.4824.pdf">- Fast global convergence of gradient methods for high-dimensional statistical recovery</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="http://jmlr.org/papers/volume18/14-415/14-415.pdf">- Gradient hard thresholding pursuit</a></b></td>  
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1316092865">- High-dimensional covariance estimation by minimizing \ell_1-penalized log-determinant divergence</a></b></td>  
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://www.sciencedirect.com/science/article/pii/S1063520308000638">- CoSaMP: Iterative signal recovery from incomplete and inaccurate samples</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1703.03208.pdf">- Compressed sensing using generative models</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="http://proceedings.mlr.press/v37/hegde15.pdf">- A Nearly-Linear Time Framework for Graph-Structured Sparsity</a></b></td>  
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1711.10925.pdf">- Deep image prior</a></b></td>  
  </tr>
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 6.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><a href="https://arxiv.org/abs/1712.07897f"><b>- Non-convex optimization for machine learning (Section 8)</b></a>, Prateek Jain, Purushottam Kar</td>
  </tr>
    
  <br>
  
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="https://www.nature.com/articles/s41534-018-0080-4">- Provable quantum state tomography via non-convex methods</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://people.eecs.berkeley.edu/~brecht/papers/07.rfp.lowrank.pdf">- Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1103.2816.pdf">- Universal low-rank matrix recovery from Pauli measurements</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/abs/1009.2065">- Templates for convex cone problems with applications to sparse signal recovery</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/abs/1203.4481">- Matrix Recipes for Hard Thresholding Methods</a></b></td> 
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/abs/1606.03168">- Finding Low-Rank Solutions via Non-Convex Matrix Factorization, Efficiently and Provably
</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/abs/1509.03917">- Dropping Convexity for Faster Semi-definite Optimization</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/abs/1509.03025">- Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.408.1839&rep=rep1&type=pdf">- Neural networks and PCA: Learning from examples without local minima</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://papers.nips.cc/paper/5430-non-convex-robust-pca.pdf">- Provable non-convex robust PCA</a></b></td>  
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="http://proceedings.mlr.press/v23/spielman12/spielman12.pdf">- Exact Recovery of Sparsely-Used Dictionaries</a></b></td>  
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1710.05092.pdf">- Dropout as a Low-Rank Regularizer for Matrix Factorization</a></b></td>  
  </tr> 
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 7.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><a href="https://arxiv.org/abs/1712.07897f"><b>- Non-convex optimization for machine learning (Section 6)</b></a>, Prateek Jain, Purushottam Kar</td>
  </tr>
    
  <br>
  
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1412.0233.pdf">- The loss sufraces of multilinear networks</a></b></td> 
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://papers.nips.cc/paper/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization.pdf">- Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1503.02101.pdf">- Escaping From Saddle Points -- Online Stochastic Gradient for Tensor Decomposition</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.offconvex.org/2016/03/22/saddlepoints/">- Escaping from saddle points</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.argmin.net/2016/03/24/saddles-again/">- Saddles again</a></b></td> 
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.argmin.net/2016/04/11/flatness/">- The hardest part
</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="http://proceedings.mlr.press/v49/lee16.pdf">- Gradient Descent Only Converges to Minimizers
</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1510.06096.pdf">- When are nonconvex problems not scary?</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1605.07272.pdf">- Matrix Completion has No Spurious Local Minimum
</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1605.07221.pdf">- Global Optimality of Local Search for Low Rank Matrix Recovery</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1609.03240.pdf">- Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach</a></b></td>  
  </tr> 
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 8.</b></td>
  </tr>    
  <br>
  
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf">- Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="http://proceedings.mlr.press/v70/tandon17a/tandon17a.pdf">- Gradient Coding: Avoiding Stragglers in Distributed Learning</a></b></td> 
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://ai.google/research/pubs/pub40565">- Large Scale Distributed Deep Networks</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1507.06970.pdf">- Perturbed Iterate Analysis for Asynchronous Stochastic Optimization</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1605.09721.pdf">- CYCLADES: Conflict-free Asynchronous Machine Learning</a></b></td> 
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="http://hyperopt.github.io/hyperopt/">- Hyperopt: Distributed Asynchronous Hyperparameter Optimization in Python</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://papers.nips.cc/paper/4006-parallelized-stochastic-gradient-descent.pdf">- Parallelized Stochastic Gradient Descent</a></b></td> 
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/0911.0491.pdf">- Slow learners are fast</a></b></td> 
  </tr>
</table>


<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Presentations #2</b></td>
    <td align="left"><b><a href="http://www.stat.ucdavis.edu/~chohsieh/wildSGD.pdf">- HogWild++: A New Mechanism for Decentralized Asynchronous Stochastic Gradient Descent</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1809.10505.pdf">- The Convergence of Sparsified Gradient Methods</a></b></td>   
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1706.02677.pdf">- Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a></b></td>  
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1604.00981.pdf">- Revisiting Distributed Synchronous SGD</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1610.02132.pdf">- QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding</a></b></td>    
  </tr> 
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 9.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><a href="https://www.deeplearningbook.org/"><b>- Deep learning book (Chapter 8)</b></a>, Ian Goodfellow and Yoshua Bengio and Aaron Courville</td>
  </tr>   
  <br>
  
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1309.5549.pdf">- Stochastic First- and Zeroth-order Methods for Nonconvex Stochastic Programming</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1611.00756.pdf">- Accelerated Methods for Non-Convex Optimization</a></b></td> 
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1704.08227.pdf">- Accelerating Stochastic Gradient Descent For Least Squares Regression</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1803.05591.pdf">- On the insufficiency of existing momentum schemes for Stochastic Optimization</a></b></td>  
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1707.02670.pdf">- Accelerated Stochastic Power Iteration</a></b></td> 
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">- Adaptive Subgradient Methods for
Online Learning and Stochastic Optimization</a></b></td>  
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="http://akyrillidis.github.io/notes/AdaGrad">- The AdaGrad algorithm</a></b></td> 
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/abs/1412.6980">- Adam: A Method for Stochastic Optimization
</a></b></td> 
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="http://ruder.io/optimizing-gradient-descent/">- An overview of gradient descent optimization algorithms</a></b></td> 
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://towardsdatascience.com/how-to-train-neural-network-faster-with-optimizers-d297730b3713">- How to train Neural Network faster with optimizers?</a></b></td> 
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1705.08292.pdf">- The Marginal Value of Adaptive Gradient Methods in Machine Learning</a></b></td> 
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1811.07055.pdf">- Minimum norm solutions do not always generalize well for over-parameterized problems</a></b></td> 
  </tr>
</table>
