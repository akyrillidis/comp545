<h2 align="center"><b> Advanced topics in optimization: From simple to complex ML systems</b> </h2>

<br>
<br>

<table style="width:100%">  
  <tr>
    <td>Email (instructor): anastasios@rice.edu</td>
    <td align="right">Web: https://akyrillidis.github.io/comp545/</td> 
  </tr>
  <tr>
    <td> </td>
    <td align="right">Email (course): RiceCOMP545@gmail.com</td> 
  </tr>
  <tr>
    <td>Office hours: XX:XXam </td>
    <td align="right">Class hours: T\TH 14:30 - 15:45</td> 
  </tr>
  <tr>
    <td>Office: DH 3119</td>
    <td align="right">Classroom: XXXX </td> 
  </tr>
</table>

<br>

<table style="width:100%">  
  <tr> 
    <td align="left"><a href="http://akyrillidis.github.io/comp545/">Course description</a></td>
    <td align="left"><a href="http://akyrillidis.github.io/comp545/schedule.html">Schedule</a></td> 
    <td align="left"><a href="http://akyrillidis.github.io/comp545/grading.html">Grading policy</a></td> 
    <td align="left"><a href="http://akyrillidis.github.io/comp545/literature.html">Literature</a></td> 
  </tr>
</table>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 1.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><b>- Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares</b></td>
  </tr>
  <tr>
    <td></td>
    <td align="left"><b>Stephen Boyd, Lieven Vandenberghe</b></td>
  </tr>
  <br>
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="https://stanford.edu/~jduchi/projects/DuchiShSiCh08.pdf">- Efficient Projections onto the \ell_1-Ball for Learning in High Dimensions</a></b></td>    
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="http://proceedings.mlr.press/v37/asteris15.pdf">- Stay on path: PCA along graph paths</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.pnas.org/content/106/3/697.full.pdf">- CUR matrix decompositions for improved data analysis</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1206.0594.pdf">- Simple and Deterministic Matrix Sketching</a></b></td> 
  </tr> 
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 2.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><b>- Introductory lectures on convex optimization (Sections 1.2, 1.3, 2.1)</b></td>
  </tr>
  <tr>
    <td></td>
    <td align="left"><b>Yurii Nesterov</b></td>
  </tr>
  
  <tr>
    <td></td>
    <td align="left"><b><a href="http://sbubeck.com/Bubeck15.pdf">- Convex Optimization: Algorithms and Complexity (Sections 3.1, 3.2, 3.4, 3.5)</a></b></td>
  </tr>
  <tr>
    <td></td>
    <td align="left"><b>Sebastien Bubeck</b></td>
  </tr>
  
  <br>
  
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="https://arxiv.org/abs/1608.04636">- Linear convergence of gradient and proximal-gradient methods under the Polyak-≈Åojasiewicz condition</a></b></td>    
  </tr>  
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 3.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><b>- Introductory lectures on convex optimization (Sections 1.2, 2.2)</b></td>
  </tr>
  <tr>
    <td></td>
    <td align="left"><b>Yurii Nesterov</b></td>
  </tr>
  
  <tr>
    <td></td>
    <td align="left"><b>- Numerical optimization (Sections 8.1, 8.2) </b></td>
  </tr>
  <tr>
    <td></td>
    <td align="left"><b>Jorge Nocedal and Stephen Wright</b></td>
  </tr>
  
  <tr>
    <td></td>
  <td align="left"><b><a href="https://leon.bottou.org/publications/pdf/tr-optml-2016.pdf">- Optimization Methods for Large-Scale Machine Learning</a></b></td>
  </tr>
  <tr>
    <td></td>
    <td align="left"><b>Leon Bottou, Frank Curtis, and Jorge Nocedal</b></td>
  </tr>
  
  <br>
  
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="http://www.ams.org/journals/mcom/1970-24-109/S0025-5718-1970-0258249-6/S0025-5718-1970-0258249-6.pdf">- A Family of Variable-Metric Methods Derived by Variational Means</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="ftp://ftp.numerical.rl.ac.uk/pub/nimg/pubs_old/ConnGoulToin91_mp.pdf">- Convergence of quasi-Newton matrices generated by the symmetric rank one update</a></b></td>   
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://pdfs.semanticscholar.org/8427/2faaaf0074b461570e5bb48514ac2c94aa72.pdf">- Random Gradient-Free Minimization of Convex Functions</a></b></td>  
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1802.05666.pdf">- Adversarial Risk and the Dangers of Evaluating Against Weak Attacks</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://www.sciencedirect.com/science/article/abs/pii/0041555364901375">- Some methods of speeding up the convergence of iteration methods</a></b></td>   
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="http://mpawankumar.info/teaching/cdt-big-data/nesterov83.pdf">- A method for solving the convex programming problem with convergence rate O(1/k^2)</a></b></td>     
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://statweb.stanford.edu/~candes/papers/adap_restart_paper.pdf">- Adaptive Restart for Accelerated Gradient Schemes</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://distill.pub/2017/momentum/">- Why Momentum Really Works</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="http://tongzhang-ml.org/papers/icml04-stograd.pdf">- Solving large-scale linear prediction problems using SGD</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1310.5715.pdf">- Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.optimization-online.org/DB_FILE/2014/12/4679.pdf">- Coordinate descent algorithms</a></b></td>    
  </tr> 
  
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 4.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><b>- Introductory lectures on convex optimization (Sections 1.2, 2.1, 2.2)</b></td>
  </tr>
  <tr>
    <td></td>
    <td align="left"><b>Yurii Nesterov</b></td>
  </tr>
    
  <br>
  
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf">- Stochastic Gradient Descent Tricks</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf">- Early stopping - but when?</a></b></td>  
  </tr>  
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Presentations #1</b></td>
    <td align="left"><b><a href="http://m8j.net/math/revisited-FW.pdf">- Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization</a></b></td>    
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf">- Deep learning via Hessian-free optimization</a></b></td>   
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf">- On the importance of initialization and momentum in deep learning</a></b></td>  
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1609.04836.pdf">- On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a></b></td>    
  </tr>  
</table>

<hr/>
