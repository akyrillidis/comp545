<h2 align="center"><b> Advanced topics in optimization: From simple to complex ML systems</b> </h2>

<br>
<br>

<table style="width:100%">  
  <tr>
    <td>Email (instructor): anastasios@rice.edu</td>
    <td align="right">Web: https://akyrillidis.github.io/comp545/</td> 
  </tr>
  <tr>
    <td> </td>
    <td align="right">Email (course): RiceCOMP545@gmail.com</td> 
  </tr>
  <tr>
    <td>Office hours: By appointment </td>
    <td align="right">Class hours: T\TH 15:10 - 16:30</td> 
  </tr>
  <tr>
    <td>Office: DH 3119</td>
    <td align="right">Classroom: Online </td> 
  </tr>
</table>

<table style="width:100%">  
  <tr> 
    <td align="center"><a href="./Syllabus.pdf">Course Syllabus</a></td>
    <td align="center"><a href="./scribe_template.zip">LaTEX template for scribing</a></td>
  </tr>
</table>

<table style="width:100%">  
  <tr> 
    <td align="left"><a href="http://akyrillidis.github.io/comp545/">Course description</a></td>
    <td align="left"><a href="http://akyrillidis.github.io/comp545/schedule.html">Schedule</a></td> 
    <td align="left"><a href="http://akyrillidis.github.io/comp545/grading.html">Grading policy</a></td> 
    <td align="left"><a href="http://akyrillidis.github.io/comp545/literature.html">Literature</a></td> 
  </tr>
</table>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 1.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><b> (None) </b></td>
  </tr>
  <br>
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1707.06347.pdf">- Proximal policy optimization algorithms</a></b></td>    
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://openai.com/blog/openai-five/">- OpenAI Five</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">- AlphaStar: Mastering the Real-Time Strategy Game StarCraft II</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1903.07291.pdf">- Semantic Image Synthesis with Spatially-Adaptive Normalization</a></b></td> 
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://www.youtube.com/watch?v=p5U4NgVGAwg">- GauGAN: Changing Sketches into Photorealistic Masterpieces </a></b></td> 
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://advances.sciencemag.org/content/5/4/eaav2372">- Combinatorial optimization by simulating adiabatic bifurcations in nonlinear Hamiltonian systems </a></b></td> 
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://www.nature.com/articles/s41586-019-1666-5">- Quantum supremacy using a programmable superconducting processor </a></b></td> 
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://www.ibm.com/blogs/research/2019/10/on-quantum-supremacy/">- On "Quantum" supremacy </a></b></td> 
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="nature.com/articles/d41586-020-03434-7">- Physicists in China challenge Google's 'quantum advantage' </a></b></td> 
  </tr> 

</table>

<hr/>


<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 2.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><b> (None) </b></td>
  </tr>
  <br>
  <tr>
    <td>Other references: </td>
    <td align="left"><b><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">- Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></b></td>    
  </tr>  
  <tr>
    <td></td>
    <td align="left"><b><a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">- RMSProp algorithm</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1412.6980.pdf">- Adam: A method for stochastic optimization</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1904.09237.pdf">- On the convergence of adam and beyond</a></b></td> 
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1910.04952.pdf">- Decaying momentum helps neural network training </a></b></td> 
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1810.06801.pdf">- Quasi-hyperbolic momentum and Adam for deep learning
 </a></b></td> 
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1804.00325.pdf">- Aggregated Momentum: Stability Through Passive Damping </a></b></td> 
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://ruder.io/optimizing-gradient-descent/">- An overview of gradient descent optimization algorithms</a></b></td> 
  </tr> 

</table>

<hr/>


<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 3.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><b><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">- Convex optimization (Chapter 5), Stephen Boyd and Lieven Vandenberghe</a></b></td>
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://mitpress.mit.edu/books/algorithms-optimization">- Algorithms for optimization (Chapter 10), Mykel Kochenderfer and Time Wheeler</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">- Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers, S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein
</a></b></td>    
  </tr> 
  <br>
  <tr>
    <td>Other references: </td>
    <td align="left"><b> (None) </b></td>  
  </tr>  

</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 4.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><b><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">- Convex optimization (Chapter 9.5-9.7, Chapter 11), Stephen Boyd and Lieven Vandenberghe</a></b></td>
  </tr>
  <tr>
    <td></td>
    <td align="left"><b><a href="https://theory.epfl.ch/vishnoi/Nisheeth-VishnoiFall2014-ConvexOptimization.pdf">- A mini-course on convex optimization (Chapter 3), Nisheeth Vishnoi</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1405.4980.pdf">- Convex Optimization: Algorithms and Complexity (Chapter 5.3), Sebastien Bubeck. </a></b></td>    
  </tr> 
  <br>
  <tr>
    <td>Other references: </td>
    <td align="left"><b> (None) </b></td>  
  </tr>  

</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 5.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><b><a href="https://theory.epfl.ch/vishnoi/Nisheeth-VishnoiFall2014-ConvexOptimization.pdf">- A mini-course on convex optimization (Chapter 2), Nisheeth Vishnoi</a></b></td>    
  </tr> 
  <tr>
    <td></td>
    <td align="left"><b><a href="https://arxiv.org/pdf/1405.4980.pdf">- Convex Optimization: Algorithms and Complexity (Chapter 4), Sebastien Bubeck. </a></b></td>    
  </tr> 
  <br>
  <tr>
    <td>Other references: </td>
    <td align="left"><b> <a href="https://www.cs.princeton.edu/~arora/pubs/MWsurvey.pdf">- The Multiplicative Weights Update Method: a Meta Algorithm and Applications</a> </b></td>  
  </tr>  
</table>

<hr/>

<table style="width:100%">  
  <col width="25%">
  <col width="75%">  
  <tr>
    <td><b>Lecture 6.</b></td>
  </tr>
  <tr>
    <td>Textbook: </td>
    <td align="left"><b> (None) </b></td>    
  </tr> 
  <br>
  <tr>
    <td>Other references: </td>
    <td align="left"><b> <a href="https://adversarial-ml-tutorial.org/">- Adversarial Robustness - Theory and Practice</a> </b></td>  
  </tr>  

</table>

<hr/>

<br>
<br>
<br>

